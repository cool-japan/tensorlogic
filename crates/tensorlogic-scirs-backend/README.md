# tensorlogic-scirs-backend

**Production-Ready SciRS2-Powered Tensor Execution Backend for TensorLogic**

[![Crate](https://img.shields.io/badge/crates.io-tensorlogic--scirs--backend-orange)](https://crates.io/crates/tensorlogic-scirs-backend)
[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://docs.rs/tensorlogic-scirs-backend)
[![Tests](https://img.shields.io/badge/tests-104%2F104-brightgreen)](#)
[![Production](https://img.shields.io/badge/status-production_ready-success)](#)

## Overview

Production-ready execution backend that runs `EinsumGraph` computations using **SciRS2** (Scientific Computing in Rust v2) for high-performance CPU/SIMD tensor operations.

**Input:** `EinsumGraph` from tensorlogic-compiler
**Output:** Computed tensor values with full autodiff support

## Quick Start

```rust
use tensorlogic_scirs_backend::Scirs2Exec;
use tensorlogic_infer::TlAutodiff;
use tensorlogic_compiler::compile_to_einsum;
use tensorlogic_ir::{TLExpr, Term};

// Define a rule: knows(x, y)
let rule = TLExpr::pred("knows", vec![Term::var("x"), Term::var("y")]);

// Compile to execution graph
let graph = compile_to_einsum(&rule)?;

// Create executor and provide input tensor
let mut executor = Scirs2Exec::new();
let knows_matrix = Scirs2Exec::from_vec(
    vec![1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],
    vec![3, 3]
)?;
executor.add_tensor("knows[ab]", knows_matrix);

// Execute forward pass
let result = executor.forward(&graph)?;

// Backward pass for training
let grad_out = Scirs2Exec::ones(result.shape().to_vec())?;
let mut grads = std::collections::HashMap::new();
grads.insert("output", grad_out);
let input_grads = executor.backward(&graph, grads)?;
```

## Key Features

### ‚úÖ Execution Engine
- **Real Execution**: Full implementation of forward pass with all operations
- **Autodiff**: Production-ready backward pass with gradient computation
- **Einsum Operations**: Matrix multiplication, tensor contractions via scirs2-linalg
- **Element-wise Ops**: Unary (ReLU, Sigmoid, OneMinus) and Binary (Add, Sub, Mul, Div, Comparisons)
- **Reductions**: Sum, Max, Min, Mean, Product over specified axes
- **Logical Ops**: AND, OR (Max/ProbSum), NAND, NOR, XOR, FORALL

### ‚úÖ Performance
- **Parallel Execution**: Multi-threaded graph execution with Rayon (requires `parallel` feature)
- **Memory Pooling**: Shape-based tensor reuse with statistics tracking
- **Operation Fusion**: Analysis and optimization opportunity detection
- **SIMD Support**: Vectorized operations via feature flags
- **Batch Execution**: Parallel processing for multiple inputs

### ‚úÖ Reliability
- **Error Handling**: Comprehensive error types (ShapeMismatch, Numerical, Device, etc.)
- **Execution Tracing**: Multi-level debugging (Error/Warn/Info/Debug/Trace)
- **Numerical Stability**: Fallback mechanisms for NaN/Inf handling
- **Shape Validation**: Runtime shape inference and verification
- **Gradient Checking**: Numeric verification for autodiff correctness

### ‚úÖ Testing
- **131 Tests**: All passing with comprehensive coverage
- **Property-Based**: 11 proptest tests for mathematical properties
- **Gradient Tests**: Numeric gradient checking verifies autodiff accuracy
- **Integration Tests**: End-to-end TLExpr ‚Üí Graph ‚Üí Execution
- **Parallel Tests**: 8 tests for multi-threaded execution

## Architecture

```
EinsumGraph (from compiler)
  ‚Üì
Scirs2Exec::forward()
  ‚Üì
For each EinsumNode (topological order):
  - Einsum ‚Üí scirs2_linalg::einsum() [tensor contraction]
  - ElemUnary ‚Üí ReLU/Sigmoid/OneMinus
  - ElemBinary ‚Üí Add/Sub/Mul/Div/Comparisons
  - Reduce ‚Üí Sum/Max/Min/Mean/Product over axes
  ‚Üì
TensorOutput (scirs2-core ArrayD<f64>)
  ‚Üì
Scirs2Exec::backward() [optional, for training]
  ‚Üì
Gradients (for each input tensor)
```

## Supported Operations

### 1. Einsum (Tensor Contraction)
```rust
// Matrix multiplication: C = AB
// Compiled as einsum("ik,kj->ij", A, B)
let a = Scirs2Exec::from_vec(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0], vec![2, 3])?;
let b = Scirs2Exec::from_vec(vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0], vec![3, 2])?;
// Result via graph execution: 2x2 matrix
```

### 2. Unary Operations
```rust
// ReLU: max(0, x)
// Sigmoid: 1 / (1 + exp(-x))
// OneMinus: 1 - x

// Gradient support:
// - ReLU: grad * (input > 0)
// - Sigmoid: grad * sigmoid(x) * (1 - sigmoid(x))
```

### 3. Binary Operations
```rust
// Arithmetic: Add, Subtract, Multiply, Divide
// Comparisons: Eq, Lt, Gt, Lte, Gte (return 0.0 or 1.0)
// Logical: AND (multiply), OR (max or prob_sum), XOR, NAND, NOR

// All with proper gradient computation
```

### 4. Reductions
```rust
// Sum, Max, Min, Mean, Product over specified axes
// With gradient broadcasting back to original shape

// Example: Sum over axis 1
// Input: [3, 4] ‚Üí Output: [3]
// Gradient: [3] ‚Üí broadcasted to [3, 4] (all ones)
```

## Advanced Features

### Error Handling
```rust
use tensorlogic_scirs_backend::{TlBackendError, TlBackendResult};

// Comprehensive error types
match result {
    Err(TlBackendError::ShapeMismatch(err)) => {
        println!("Shape error: {}", err);
    }
    Err(TlBackendError::NumericalError(err)) => {
        println!("Numerical issue: {:?}", err.kind);
    }
    Err(TlBackendError::DeviceError(err)) => {
        println!("Device error: {}", err);
    }
    Ok(value) => { /* success */ }
}
```

### Execution Tracing
```rust
use tensorlogic_scirs_backend::{ExecutionTracer, TraceLevel};

// Enable detailed tracing
let mut tracer = ExecutionTracer::new(TraceLevel::Debug);

// Operations are automatically traced
// Access trace events
for event in tracer.events() {
    println!("{}", event);  // Shows operation, duration, inputs/outputs
}

// Get statistics
let stats = tracer.stats();
println!("Total ops: {}", stats.total_operations);
println!("Total time: {:?}", stats.total_duration);
```

### Numerical Stability
```rust
use tensorlogic_scirs_backend::{FallbackConfig, sanitize_tensor};

// Configure fallback behavior
let config = FallbackConfig::permissive()
    .with_nan_replacement(0.0)
    .with_inf_replacement(1e10, -1e10);

// Sanitize tensors before operations
let clean_tensor = sanitize_tensor(&input, &config, "my_operation")?;

// Safe operations
use tensorlogic_scirs_backend::fallback::{safe_div, safe_log, safe_sqrt};
let result = safe_div(&a, &b, 1e-10);  // Avoids division by zero
```

### Memory Pooling
```rust
use tensorlogic_scirs_backend::Scirs2Exec;

// Enable memory pooling
let mut executor = Scirs2Exec::new();
executor.enable_pooling();

// Check pooling statistics
let stats = executor.pool_stats();
println!("Reuse rate: {:.1}%", stats.reuse_rate * 100.0);
```

### Gradient Verification
```rust
use tensorlogic_scirs_backend::gradient_check::{check_gradients, GradientCheckConfig};

// Verify gradient correctness
let config = GradientCheckConfig::default()
    .with_epsilon(1e-5)
    .with_rtol(1e-4)
    .with_atol(1e-6);

let report = check_gradients(&graph, &executor, &config)?;

if report.all_passed {
    println!("All gradients correct!");
} else {
    for result in &report.results {
        println!("{}: max_error = {:.2e}", result.tensor_name, result.max_abs_diff);
    }
}
```

### Parallel Execution

**Requires**: `parallel` feature flag

Multi-threaded execution automatically detects independent operations and executes them in parallel using Rayon.

```toml
[dependencies]
tensorlogic-scirs-backend = { version = "0.1", features = ["parallel"] }
```

#### Basic Usage

```rust
use tensorlogic_scirs_backend::ParallelScirs2Exec;
use tensorlogic_infer::TlAutodiff;

// Create parallel executor
let mut executor = ParallelScirs2Exec::new();

// Optional: Configure thread pool
executor.set_num_threads(4);

// Add input tensors
executor.add_tensor("p1", tensor1);
executor.add_tensor("p2", tensor2);

// Execute with automatic parallelization
let result = executor.forward(&graph)?;

// Check parallelization statistics
if let Some(stats) = executor.execution_stats() {
    println!("Parallel ops: {}", stats.parallel_ops);
    println!("Sequential ops: {}", stats.sequential_ops);
    println!("Estimated speedup: {:.2}x", stats.estimated_speedup);
}
```

#### Advanced Configuration

```rust
use tensorlogic_scirs_backend::{ParallelConfig, ParallelScirs2Exec};

// Custom configuration
let config = ParallelConfig {
    num_threads: Some(8),          // Use 8 threads (None = all cores)
    min_parallel_ops: 3,            // Minimum ops per level for parallelization
    enable_pooling: true,           // Enable memory pooling
};

let mut executor = ParallelScirs2Exec::with_config(config);

// Execute as normal
let result = executor.forward(&graph)?;
```

#### How It Works

The parallel executor:
1. **Analyzes dependencies** between operations in the graph
2. **Groups operations** into execution levels (topologically sorted)
3. **Executes each level** with operations running in parallel using Rayon
4. **Optimizes overhead** by running small levels sequentially

**Example Graph:**
```
Op0: c = relu(a)     ‚îÇ Level 0: Execute Op0 and Op1 in parallel
Op1: d = sigmoid(b)  ‚îÇ
Op2: e = c + d       ‚îÇ Level 1: Execute Op2 sequentially
Op3: f = relu(e)     ‚îÇ Level 2: Execute Op3 sequentially
```

#### Performance Characteristics

- **Best speedup**: Graphs with many independent operations (e.g., `AND(p1, p2, p3, p4)`)
- **No speedup**: Sequential chains (e.g., `EXISTS(j, NOT(P))`)
- **Overhead threshold**: Operations below `min_parallel_ops` run sequentially
- **Backward pass**: Currently sequential (dependencies more complex)

#### Benchmarking

```bash
# Run parallel performance benchmarks
cargo bench --bench parallel_performance --features parallel

# Compare sequential vs parallel
cargo bench --bench parallel_performance --features parallel -- "high_parallelism"
```

## Backend Features

### CPU Backend (Default)
```toml
[dependencies]
tensorlogic-scirs-backend = "0.1"
```

### SIMD Backend (Faster)
```toml
[dependencies]
tensorlogic-scirs-backend = { version = "0.1", features = ["simd"] }
```

Enables vectorized operations for element-wise ops and reductions.

### Parallel + SIMD (Best Performance)
```toml
[dependencies]
tensorlogic-scirs-backend = { version = "0.1", features = ["parallel", "simd"] }
```

Combines multi-threaded execution with SIMD vectorization for maximum performance.

### GPU Backend (Future)
```toml
[dependencies]
tensorlogic-scirs-backend = { version = "0.1", features = ["gpu"] }
```

## Advanced Backend Features

### Execution Modes

The backend supports multiple execution modes for different performance/debugging tradeoffs:

```rust
use tensorlogic_scirs_backend::{ExecutionMode, ExecutionConfig, Scirs2Exec};

// Eager mode (default) - immediate execution
let config = ExecutionConfig::eager();

// Graph mode - compile and optimize before execution
let config = ExecutionConfig::graph()
    .with_optimizations(true)
    .with_memory_planning(true);

// JIT mode (future) - compile to native code
// let config = ExecutionConfig::jit();
```

**Graph Compilation Example:**

```rust
use tensorlogic_scirs_backend::execution_mode::CompiledGraph;

// Compile a graph for optimized execution
let compiled = CompiledGraph::compile(graph);

// View compilation statistics
println!("Original ops: {}", compiled.stats().original_ops);
println!("Optimized ops: {}", compiled.stats().optimized_ops);
println!("Compilation time: {:.2}ms", compiled.stats().compilation_time_ms);

// Execute the optimized graph
let result = executor.forward(compiled.graph())?;
```

### Device Management

Manage compute devices (CPU/GPU) with the device API:

```rust
use tensorlogic_scirs_backend::{DeviceManager, Device, DeviceType};

// Query available devices
let manager = DeviceManager::new();
println!("Available devices: {:?}", manager.available_devices());

// Check for GPU availability
if manager.has_gpu() {
    println!("GPU devices found: {}", manager.count_devices(DeviceType::Cuda));
}

// Select a specific device
let device = Device::cuda(0); // CUDA GPU 0
let device = Device::cpu();    // CPU
let device = Device::metal();  // Apple Metal

// Check if device is available
if manager.is_available(&device) {
    manager.set_default_device(device)?;
}
```

**Supported Device Types:**
- **CPU**: Always available, default
- **CUDA**: NVIDIA GPUs (future)
- **Metal**: Apple GPUs (future)
- **Vulkan**: Cross-platform compute (future)
- **ROCm**: AMD GPUs (future)

### Precision Control

Control numerical precision for memory/speed tradeoffs:

```rust
use tensorlogic_scirs_backend::{Precision, PrecisionConfig, Scalar};

// Different precision modes
let config = PrecisionConfig::f32();  // 32-bit (faster, less memory)
let config = PrecisionConfig::f64();  // 64-bit (more accurate, default)
let config = PrecisionConfig::mixed_precision(); // Mixed 16/32-bit

// Configure mixed precision training
let config = PrecisionConfig::mixed_precision()
    .with_loss_scale(2048.0)
    .with_dynamic_loss_scaling(true);

// Query precision properties
println!("Precision: {}", Precision::F32);
println!("Memory savings: {:.1}%", Precision::F32.memory_savings() * 100.0);
```

**Precision Options:**
- **F32**: 32-bit floating point (50% memory savings vs F64)
- **F64**: 64-bit floating point (default, maximum accuracy)
- **Mixed16**: FP16 storage, FP32 compute (75% memory savings)
- **BFloat16**: BF16 storage, FP32 compute (75% memory savings)

**Generic Scalar Operations:**

The `Scalar` trait abstracts over f32/f64:

```rust
use tensorlogic_scirs_backend::Scalar;

fn compute<T: Scalar>(x: T, y: T) -> T {
    x.sqrt() + y.exp()
}

let result_f32 = compute(2.0f32, 1.0f32);
let result_f64 = compute(2.0f64, 1.0f64);
```

## SciRS2 Integration

This crate strictly adheres to the SciRS2 integration policy:

```rust
// ‚úì Correct: Use SciRS2
use scirs2_core::ndarray::{Array, ArrayD, Axis};
use scirs2_core::array;
use scirs2_linalg::einsum;

// ‚úó Wrong: Never import these directly
use ndarray::Array2;  // ‚ùå
use rand::thread_rng;  // ‚ùå
use num_complex::Complex64;  // ‚ùå
```

All tensor operations, linear algebra, and future autograd features use SciRS2.

## Testing

```bash
# Run all tests
cargo nextest run -p tensorlogic-scirs-backend

# Run with SIMD
cargo nextest run -p tensorlogic-scirs-backend --features simd

# Run with parallel execution
cargo nextest run -p tensorlogic-scirs-backend --features parallel

# Run property tests
cargo test -p tensorlogic-scirs-backend --test proptests

# Run benchmarks
cargo bench -p tensorlogic-scirs-backend

# Run parallel benchmarks
cargo bench -p tensorlogic-scirs-backend --bench parallel_performance --features parallel
```

### Test Coverage

**152 tests, all passing:**
- **120 unit tests**: Core functionality (einsum, operations, reductions, parallel execution, backend features)
- **14 integration tests**: End-to-end TLExpr ‚Üí Graph ‚Üí Execution
- **7 logical ops tests**: Extended operations (OR, NAND, NOR, XOR)
- **11 property tests**: Mathematical properties (commutativity, associativity, etc.)

**Module breakdown:**
- autodiff, executor, ops: Core execution and gradient computation
- parallel_executor: Multi-threaded execution (8 tests)
- memory_pool: Tensor reuse and pooling (7 tests)
- dependency_analyzer: Graph analysis for parallelization (8 tests)
- gradient_ops: Advanced gradient estimators (12 tests)
- error, tracing, fallback: Reliability features (29 tests)
- execution_mode, device, precision: Backend features (21 tests)

### Property-Based Testing

Uses proptest to verify mathematical properties:
- Addition commutativity: `a + b = b + a`
- Multiplication associativity: `(a * b) * c = a * (b * c)`
- Distributivity: `a * (b + c) = a*b + a*c`
- Sum linearity: `sum(a*x + b*y) = a*sum(x) + b*sum(y)`
- Sigmoid range: `0 ‚â§ sigmoid(x) ‚â§ 1`
- Identity/inverse properties

## Performance

### Benchmarks

```bash
cargo bench -p tensorlogic-scirs-backend
```

Available benchmarks:
- `forward_pass`: Forward execution throughput
- `simd_comparison`: CPU vs SIMD performance
- `memory_footprint`: Memory usage tracking
- `gradient_stability`: Backward pass stability
- `throughput`: Operations per second

### Optimization Features

1. **Memory Pooling**: Reuses tensors with matching shapes (tracked statistics)
2. **Operation Fusion**: Detects fusion opportunities (analysis-only, execution pending)
3. **SIMD**: Vectorized operations via `--features simd`
4. **Batch Execution**: Parallel processing for multiple inputs

## Integration Example

Full example with training:

```rust
use tensorlogic_compiler::compile_to_einsum;
use tensorlogic_scirs_backend::Scirs2Exec;
use tensorlogic_infer::TlAutodiff;
use tensorlogic_ir::{TLExpr, Term};

// Define rule: knows(x,y) ‚àß knows(y,z) ‚Üí knows(x,z) (transitivity)
let knows_xy = TLExpr::pred("knows", vec![Term::var("x"), Term::var("y")]);
let knows_yz = TLExpr::pred("knows", vec![Term::var("y"), Term::var("z")]);
let premise = TLExpr::and(knows_xy, knows_yz);

// Compile to graph
let graph = compile_to_einsum(&premise)?;

// Setup executor with input data
let mut executor = Scirs2Exec::new();
let knows_matrix = Scirs2Exec::from_vec(
    vec![1.0, 0.0, 1.0,
         0.0, 1.0, 0.0,
         0.0, 0.0, 1.0],
    vec![3, 3]
)?;
executor.add_tensor("knows[ab]", knows_matrix);

// Forward pass
let result = executor.forward(&graph)?;
println!("Result shape: {:?}", result.shape());

// Backward pass for training
let loss_grad = Scirs2Exec::ones(result.shape().to_vec())?;
let mut grads = std::collections::HashMap::new();
grads.insert("output", loss_grad);
let input_grads = executor.backward(&graph, grads)?;

// Access gradients
for (name, grad) in input_grads.tensors.iter() {
    println!("Gradient for {}: {:?}", name, grad.shape());
}
```

## API Documentation

Key public types:

- `Scirs2Exec`: Main executor implementing `TlAutodiff` trait
- `TlBackendError`: Comprehensive error types
- `ExecutionTracer`: Debug tracing with multiple levels
- `FallbackConfig`: Numerical stability configuration
- `ForwardTape`: Stores intermediate values for backward pass
- `ParallelBatchExecutor`: Batch processing with parallelization
- `ProfiledScirs2Exec`: Performance profiling wrapper

See [full API docs](https://docs.rs/tensorlogic-scirs-backend) for details.

## Limitations & Future Work

Current limitations:
- **No GPU support**: CPU/SIMD only (GPU planned via scirs2 GPU features)
- **No JIT compilation**: Eager execution only
- **No distributed execution**: Single-device only

See [TODO.md](TODO.md) for the complete roadmap (72% complete, 65/90 tasks).

Next priorities:
- Parallelization (scirs2 parallel features)
- In-place operations (memory optimization)
- Multiple execution modes (eager/compiled/JIT)

## Contributing

When contributing:
1. Follow SciRS2 integration policy strictly
2. Add tests for all new features (maintain 100% pass rate)
3. Use `cargo clippy -- -D warnings` (zero warnings policy)
4. Format code with `cargo fmt`
5. Keep files under 2000 lines (use SplitRS if needed)
6. Update TODO.md with task status

## License

Apache-2.0

---

**Status**: üéâ Production Ready (v0.1.0-alpha.1)
**Last Updated**: 2025-11-06
**Tests**: 104/104 passing (100%)
**Completion**: 72% (65/90 tasks)
**Part of**: [TensorLogic Ecosystem](https://github.com/cool-japan/tensorlogic)
